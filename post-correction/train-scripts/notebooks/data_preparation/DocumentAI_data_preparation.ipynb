{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data_ratio = 0.2\n",
    "number_of_folds = 3\n",
    "experiment = 'bugfix_big_4x'\n",
    "synthetic_data = True\n",
    "repeat_data_number = 4\n",
    "\n",
    "# 1,3 - big\n",
    "# 2   - clean\n",
    "# 4   - messy\n",
    "# 5   - synthetic 4x big\n",
    "\n",
    "#TODO: #synthetically generate errors which we KNOW are due to incorrect line segmentation\n",
    "#TODO: # Create dataset..\n",
    "\n",
    "# 6   - synthetic 4x big\n",
    "# 7   - synthetic 4x clean\n",
    "# 8   - synthetic 4x messy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: /home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/data/experiment_bugfix_big_4x\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from aksharamukha import transliterate\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import random\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "csv_file = f'/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/data/experiment_{experiment}/combined.csv'\n",
    "directory = f'/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/data/experiment_{experiment}'\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory created: {directory}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been merged into the variable.\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'big'\n",
    "\n",
    "# Directory containing the files\n",
    "directory = f'/home/ocr_proj/OCR/annotations/{folder_name}/text'\n",
    "image_directory = f'/home/ocr_proj/OCR/annotations/{folder_name}/images'\n",
    "\n",
    "# Initialize an empty string to store the merged content\n",
    "merged_content_annot= []\n",
    "merged_content_pred = []\n",
    "image_paths = []\n",
    "\n",
    "# Iterate over the range of file numbers\n",
    "for i in range(12,52): # uncomment for big\n",
    "#for i in range(2,43): # uncomment for clean\n",
    "#for i in range(2, 17): # uncomment for messy\n",
    "    # Construct the filename\n",
    "    #if i not in [24, 29, 30, 31, 32, 33, 34, 35, 36, 37]: # uncomment for clean\n",
    "        filename_annot = f'pg{i:04d}_annotated.txt'\n",
    "        filepath_annot = os.path.join(directory, filename_annot)\n",
    "\n",
    "        filename_pred = f'pg{i:04d}.txt'\n",
    "        filepath_pred = os.path.join(directory, filename_pred)\n",
    "\n",
    "        img = f'pg{i:04d}'\n",
    "        img_path = os.path.join(image_directory, img)\n",
    "        # Check if img_path exists and is a directory\n",
    "        if os.path.exists(img_path) and os.path.isdir(img_path):\n",
    "            # List all files in img_path directory\n",
    "            files = os.listdir(img_path)\n",
    "            files.sort()\n",
    "            \n",
    "            # Iterate through all files\n",
    "            for file in files:\n",
    "                # Check if the file is an image (you can adjust this condition as per your specific image extensions)\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    # Construct the full path of the image file\n",
    "                    image_file_path = os.path.join(img_path, file)\n",
    "                    # Add the image file path to the list\n",
    "                    image_paths.append(image_file_path)\n",
    "        else:\n",
    "            print(f\"Directory '{img_path}' does not exist.\")\n",
    "        \n",
    "        # Open each file in read mode\n",
    "        with open(filepath_annot, 'r') as infile:\n",
    "            # Read the content and append it to the merged_content variable\n",
    "            merged_content_annot.extend(infile.read().replace('||','॥').replace('।।','॥').replace('|','।').splitlines())\n",
    "\n",
    "        # Open each file in read mode\n",
    "        with open(filepath_pred, 'r') as infile:\n",
    "            # Read the content and append it to the merged_content variable\n",
    "            merged_content_pred.extend(infile.read().replace('||','॥').replace('।।','॥').replace('|','।').splitlines())\n",
    "# merged_content now contains the merged text of all files\n",
    "\n",
    "merged_content_annot = [transliterate.process('Devanagari', 'IAST', i) for i in merged_content_annot if i!='']\n",
    "merged_content_pred = [transliterate.process('Devanagari', 'IAST', i) for i in merged_content_pred if i!='']\n",
    "assert len(merged_content_annot) == len(merged_content_pred) == len(image_paths)\n",
    "print(\"All files have been merged into the variable.\")\n",
    "\n",
    "data = zip(merged_content_pred, merged_content_annot, image_paths)\n",
    "# Repeat each row 4 times\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file, delimiter=';')\n",
    "    # Write header row\n",
    "    writer.writerow(['input_text', 'target_text', 'path'])\n",
    "    # Write data rows\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = {\n",
    "\"('delete', 'ं')\": 102,\n",
    "\"('insert', 'ु')\": 70,\n",
    "\"('insert', 'ं')\": 61,\n",
    "\"('insert', 'ृ')\": 52,\n",
    "\"('insert', 'े')\": 43,\n",
    "\"('insert', 'ू')\": 41,\n",
    "\"('delete', 'े')\": 30,\n",
    "\"('replace', 'ो', 'ा')\": 22\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_normalize_dict(original_dict):\n",
    "    # First, create a new dictionary with swapped action types and transliterated text\n",
    "    processed_dict = {}\n",
    "    \n",
    "    for key, value in original_dict.items():\n",
    "        # Extracting the tuple part from the string\n",
    "        parts = key.strip(\"()\").split(\", \")\n",
    "        action = parts[0].strip(\"'\")\n",
    "        text = parts[1].strip(\"'\")\n",
    "        if action == \"replace\":\n",
    "            text2 = parts[2].strip(\"'\")\n",
    "            transliterated_text = transliterate.process('Devanagari', 'IAST', text)\n",
    "            transliterated_text2 = transliterate.process('Devanagari', 'IAST', text2)\n",
    "            \n",
    "            new_key = f\"('replace', '{transliterated_text2}', '{transliterated_text}')\"\n",
    "        else:\n",
    "            # Transliterate the text\n",
    "            transliterated_text = transliterate.process('Devanagari', 'IAST', text)\n",
    "            \n",
    "            # Swap action types\n",
    "            new_action = 'delete' if action == 'insert' else 'insert'\n",
    "            \n",
    "            # Create new key with swapped action and transliterated text\n",
    "            new_key = f\"('{new_action}', '{transliterated_text}')\"\n",
    "        \n",
    "        # Add the new key-value pair to the new dictionary\n",
    "        processed_dict[new_key] = value\n",
    "    \n",
    "    # Calculate the total sum of values\n",
    "    total_value = sum(processed_dict.values())\n",
    "    \n",
    "    # Normalize the values so that they sum up to one\n",
    "    normalized_dict = {key: value / total_value for key, value in processed_dict.items()}\n",
    "    \n",
    "    return normalized_dict\n",
    "\n",
    "# Perform the transliteration\n",
    "new_dict = process_and_normalize_dict(mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combining_marks = ['्', 'ँ', 'ं', 'ः', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॢ', 'ॣ', 'े', 'ै', 'ो', 'ौ', '़','ॅ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_dependent_characters = [transliterate.process('Devanagari', 'IAST', text) for text in combining_marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_synthetic_errors(data_csv,error_dict):\n",
    "\n",
    "    \n",
    "    for index, row in data_csv.iterrows():\n",
    "        # Access the first column value\n",
    "        line = row[0]  # or row['ColumnName'] if using named columns\n",
    "        line = line.strip()\n",
    "        for key, prob in error_dict.items():\n",
    "\n",
    "            key = ast.literal_eval(key)\n",
    "            rule_type = key[0]\n",
    "            \n",
    "            c_1 = key[1]\n",
    "            \n",
    "            if rule_type == \"delete\":\n",
    "                rand_delete = (\n",
    "                    lambda c: \"\" if random.random() < prob and c == c_1 else c\n",
    "                )\n",
    "                line = \"\".join([rand_delete(c) for c in line])\n",
    "\n",
    "            elif rule_type == \"replace\":\n",
    "                        c_2 = key[2]\n",
    "                        rand_replace = (\n",
    "                            lambda c: c_2 if random.random() < prob and c == c_1 else c\n",
    "                        )\n",
    "                        line = \"\".join([rand_replace(c) for c in line])\n",
    "                        \n",
    "            elif rule_type == \"insert\":\n",
    "                line = line + \" \"\n",
    "                rand_insert = (\n",
    "                    lambda c: \"{}{}\".format(c_1, c)\n",
    "                    if random.random() < prob\n",
    "                    and c not in non_dependent_characters else c\n",
    "                )\n",
    "                line = \"\".join([rand_insert(c) for c in line])\n",
    "\n",
    "        data_csv.at[index, data_csv.columns[0]] = line.strip()\n",
    "    return data_csv\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"('insert', 'ṃ')\": 0.24228028503562946,\n",
       " \"('delete', 'u')\": 0.166270783847981,\n",
       " \"('delete', 'ṃ')\": 0.14489311163895488,\n",
       " \"('delete', 'ṛ')\": 0.12351543942992874,\n",
       " \"('delete', 'e')\": 0.1021377672209026,\n",
       " \"('delete', 'ū')\": 0.09738717339667459,\n",
       " \"('insert', 'e')\": 0.07125890736342043,\n",
       " \"('replace', 'ā', 'o')\": 0.052256532066508314}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict #synthetically generate errors which we KNOW are due to incorrect line segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1989772/2847755772.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  line = row[0]  # or row['ColumnName'] if using named columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into folds with train-validation-test splits.\n"
     ]
    }
   ],
   "source": [
    "#creating cross validation dasplits\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "#csv_file = f'/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/data/{experiment}/combined.csv'  # Replace with your CSV file path\n",
    "data = pd.read_csv(csv_file, delimiter=';',encoding='utf-8')\n",
    "\n",
    "# Initialize KFold with 10 splits\n",
    "kf = KFold(n_splits=number_of_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Counter for fold iteration\n",
    "fold_count = 1\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_val_index, test_index in kf.split(data):\n",
    "    # Split data into training + validation and test sets\n",
    "    train_val_data = data.iloc[train_val_index]\n",
    "    test_data = data.iloc[test_index]\n",
    "\n",
    "\n",
    "    # # Further split training + validation data into training and validation sets\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=train_val_data_ratio, random_state=42)\n",
    "\n",
    "    if synthetic_data:\n",
    "        _train_data = pd.concat([train_data] * repeat_data_number, ignore_index=True)\n",
    "        train_data = add_synthetic_errors(_train_data, new_dict)\n",
    "    # headers = ['input_text', 'target_text']\n",
    "    # train_data.columns = headers\n",
    "    # val_data.columns = headers\n",
    "    # test_data.columns = headers\n",
    "\n",
    "    # Save the splits into separate CSV files\n",
    "    train_data.to_csv(f'/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/data/experiment_{experiment}/train_fold_{fold_count}.csv', sep=';', index=False, encoding='utf-8')\n",
    "    val_data.to_csv(f'/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/data/experiment_{experiment}/val_fold_{fold_count}.csv', sep=';', index=False, encoding='utf-8')\n",
    "    test_data.to_csv(f'/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/data/experiment_{experiment}/test_fold_{fold_count}.csv', sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "    # Increment fold count\n",
    "    fold_count += 1\n",
    "\n",
    "print('Data has been split into folds with train-validation-test splits.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code converts the raw .txt files to .csv. Change this if you want to do SLP1 instead of IAST or something..\n",
    "\n",
    "# filename = 'big'\n",
    "# # Define file names\n",
    "# file1 = f'/home/ocr_proj/OCR/post_correction/denoising/data/{filename}_src.txt'\n",
    "# file2 = f'/home/ocr_proj/OCR/post_correction/denoising/data/{filename}_tgt.txt'\n",
    "# output_csv = '/home/ocr_proj/OCR/post_correction/denoising/data/combined.csv'\n",
    "\n",
    "# # Open the two text files and the output CSV file\n",
    "# with open(file1, 'r',encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2, open(output_csv, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "#     csv_writer = csv.writer(csvfile, delimiter=';')\n",
    "\n",
    "#     # Read lines from both files\n",
    "#     lines1 = f1.readlines()\n",
    "#     lines2 = f2.readlines()\n",
    "\n",
    "    \n",
    "\n",
    "#     # Combine lines line by line\n",
    "#     for line1, line2 in zip(lines1, lines2):\n",
    "\n",
    "#         #Transliterate\n",
    "#         line1 = transliterate.process('Devanagari', 'IAST', line1)\n",
    "#         line2 = transliterate.process('Devanagari', 'IAST', line2)\n",
    "        \n",
    "#         # Write combined lines to CSV\n",
    "#         csv_writer.writerow([line1.strip(), line2.strip()])\n",
    "\n",
    "# print(f'Combined data written to {output_csv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ByT5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
