{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "#0. try to find the token used for masking\n",
    "#1. train the base model for masking\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import editdistance as ed\n",
    "device = torch.device('cuda')\n",
    "import difflib\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth            ab\u001b[1m\u001b[31mx\u001b[0mc\u001b[1m\u001b[34m \u001b[0mef\n",
      "kartik                  ab\u001b[1m\u001b[31m \u001b[0mc\u001b[1m\u001b[34md\u001b[0mef\n"
     ]
    }
   ],
   "source": [
    "def get_edit_distance(predicted_text, transcript):\n",
    "    cer = ed.eval(predicted_text, transcript) / max(len(predicted_text), len(transcript))\n",
    "    return cer\n",
    "\n",
    "def highlight_changes(line1, line2, label):\n",
    "    # Generate a diff between the two lines\n",
    "    diff = list(difflib.ndiff(line1, line2))\n",
    "\n",
    "    # Create colored output based on diff\n",
    "    highlighted_line1 = []\n",
    "    highlighted_line2 = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(diff):\n",
    "        if diff[i].startswith('- ') and i+1 < len(diff) and diff[i+1].startswith('+ '):\n",
    "            # Replace operation\n",
    "            highlighted_line1.append(colored(diff[i][2:], 'yellow', attrs=['bold']))\n",
    "            highlighted_line2.append(colored(diff[i+1][2:], 'yellow', attrs=['bold']))\n",
    "            i += 2\n",
    "        elif diff[i].startswith('- '):\n",
    "            # Delete operation\n",
    "            highlighted_line1.append(colored(diff[i][2:], 'blue', attrs=['bold']))\n",
    "            highlighted_line2.append(colored(' ', 'blue', attrs=['bold']))\n",
    "            i += 1\n",
    "        elif diff[i].startswith('+ '):\n",
    "            # Insert operation\n",
    "            highlighted_line1.append(colored(' ', 'red', attrs=['bold']))\n",
    "            highlighted_line2.append(colored(diff[i][2:], 'red', attrs=['bold']))\n",
    "            i += 1\n",
    "        elif diff[i].startswith('  '):\n",
    "            # Unchanged characters\n",
    "            highlighted_line1.append(diff[i][2:])\n",
    "            highlighted_line2.append(diff[i][2:])\n",
    "            i += 1\n",
    "        else:\n",
    "            # Skip '?' lines\n",
    "            i += 1\n",
    "\n",
    "    final_line1 = ''.join(highlighted_line1)\n",
    "    final_line2 = ''.join(highlighted_line2)\n",
    "\n",
    "    # Ensure equal length by padding with spaces\n",
    "    max_length = max(len(final_line1), len(final_line2))\n",
    "    final_line1 = final_line1.ljust(max_length)\n",
    "    final_line2 = final_line2.ljust(max_length)\n",
    "\n",
    "\n",
    "    # Pad the ground_truth_label and label_label to the same length\n",
    "    ground_truth = \"Ground Truth            \"\n",
    "    max_length_l = max(len(ground_truth), len(label))\n",
    "    padded_ground_truth = ground_truth.ljust(max_length_l)\n",
    "    padded_label = label.ljust(max_length_l)\n",
    "\n",
    "    print(f\"{padded_ground_truth}\" + final_line2)\n",
    "    print(f\"{padded_label}\" + final_line1)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "highlight_changes(\"abcdef\", \"abxcef\",'kartik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'big_7pagetrained_1x'\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n",
      "models loaded\n",
      "Directory already exists: /home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/outputs/MASK_experiment_big_7pagetrained_1x/test_fold_1\n",
      "Test csv read\n"
     ]
    }
   ],
   "source": [
    "data_path = f\"/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/data/experiment_{experiment}/test_fold_{fold}.csv\"\n",
    "# tokenizer_path = '/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/downloaded_model/models--byt5-sanskrit'\n",
    "\n",
    "#tokenizer_path = '/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/downloaded_model/models--byt5-sanskrit'\n",
    "tokenizer_path = '/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/downloaded_model/sanskrit-multitask'\n",
    "\n",
    "base_model_dir = '/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/downloaded_model/sanskrit-multitask'\n",
    "pretrained_model_dir = '/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/downloaded_model/models--byt5-sanskrit'\n",
    "finetuned_model_dir = '/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/model_checkpoints/experiment_big_7pagetrained_1x/1/checkpoint-800'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, max_length=512)\n",
    "print('tokenizer loaded')\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_dir)\n",
    "pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_dir)\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(finetuned_model_dir)\n",
    "print('models loaded')\n",
    "\n",
    "# Ensure the results directory exists\n",
    "dir_path = os.path.dirname(data_path)\n",
    "experiment_folder = os.path.basename(dir_path)\n",
    "folder_name = os.path.splitext(os.path.basename(data_path))[0]\n",
    "results_folder_path = f'/home/ocr_proj/OCR/post_correction/pe-ocr-sanskrit/outputs/MASK_{experiment_folder}/{folder_name}'\n",
    "if not os.path.exists(results_folder_path):\n",
    "    os.makedirs(results_folder_path)\n",
    "    print(f\"Directory created: {results_folder_path}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {results_folder_path}\")\n",
    "\n",
    "# Read the test CSV file\n",
    "test_df = pd.read_csv(data_path, sep=';')\n",
    "print('Test csv read')\n",
    "\n",
    "ocr_list = list(test_df.input_text.values)\n",
    "target_list = list(test_df.target_text.values)\n",
    "paths = list(test_df.path.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, text):\n",
    "   \n",
    "    tensor = torch.tensor([tokenizer(text).input_ids])\n",
    "    output_ids = model.generate(tensor,max_length=200,num_beams=3, eos_token_id=1, pad_token_id=0,decoder_start_token_id=0).tolist()\n",
    "    post_corrected_list = tokenizer.batch_decode(output_ids)\n",
    "\n",
    "    return post_corrected_list[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_number = 44\n",
    "ocr_text = ocr_list[line_number]\n",
    "tgt_text = target_list[line_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth            karaṇakatvasaṃbaṃdhenapratyayārthopradhānenvayamapekṣyasvāśrayakaraṇakatvasaṃbaṃdhenaivābhyarhitenānvayaḥsiddh\n",
      "Ground Truth            karaṇakatvasaṃbaṃdhenapratyayārthopradhānenvayamapekṣyasvāśrayakaraṇakatvasaṃbaṃdhenaivābhyarhitenānvayaḥsiddh\n",
      " \n",
      "Ground Truth            karaṇakatvasaṃbaṃdhenapratyayārthopradhān\u001b[1m\u001b[33ma\u001b[0mnvayamapekṣyasvāśraya\u001b[1m\u001b[31mṃ\u001b[0mkaraṇakatvasa\u001b[1m\u001b[34m \u001b[0mbaṃdhen\u001b[1m\u001b[31me\u001b[0m\u001b[1m\u001b[31mś\u001b[0ma\u001b[1m\u001b[31m.\u001b[0m\u001b[1m\u001b[34m \u001b[0m\u001b[1m\u001b[34m \u001b[0m\u001b[1m\u001b[34m \u001b[0mbhyarhi\u001b[1m\u001b[31mr\u001b[0m\u001b[1m\u001b[31ma\u001b[0mtenānvayaḥsid\u001b[1m\u001b[34m \u001b[0mh\u001b[1m\u001b[31me\u001b[0m\n",
      "OCR Output              karaṇakatvasaṃbaṃdhenapratyayārthopradhān\u001b[1m\u001b[33me\u001b[0mnvayamapekṣyasvāśraya\u001b[1m\u001b[31m \u001b[0mkaraṇakatvasa\u001b[1m\u001b[34mṃ\u001b[0mbaṃdhen\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0ma\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[34mi\u001b[0m\u001b[1m\u001b[34mv\u001b[0m\u001b[1m\u001b[34mā\u001b[0mbhyarhi\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0mtenānvayaḥsid\u001b[1m\u001b[34md\u001b[0mh\u001b[1m\u001b[31m \u001b[0m\n",
      " \n",
      "Ground Truth            karaṇakatvasaṃbaṃdhenapratyayārthopradhān\u001b[1m\u001b[33ma\u001b[0mnvayamapekṣyasvāśraya\u001b[1m\u001b[31mṃ\u001b[0mkaraṇakatvasa\u001b[1m\u001b[34m \u001b[0mbaṃdhen\u001b[1m\u001b[31me\u001b[0m\u001b[1m\u001b[31mś\u001b[0ma\u001b[1m\u001b[31m.\u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[34m \u001b[0m\u001b[1m\u001b[34m \u001b[0m\u001b[1m\u001b[34m \u001b[0mbhyarhi\u001b[1m\u001b[31mr\u001b[0m\u001b[1m\u001b[31ma\u001b[0mtenānvayaḥsid\u001b[1m\u001b[34m \u001b[0mh\u001b[1m\u001b[31me\u001b[0m\n",
      "Pretrained Model        karaṇakatvasaṃbaṃdhenapratyayārthopradhān\u001b[1m\u001b[33me\u001b[0mnvayamapekṣyasvāśraya\u001b[1m\u001b[31m \u001b[0mkaraṇakatvasa\u001b[1m\u001b[34mṃ\u001b[0mbaṃdhen\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0ma\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[34mi\u001b[0m\u001b[1m\u001b[34mv\u001b[0m\u001b[1m\u001b[34mā\u001b[0mbhyarhi\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0mtenānvayaḥsid\u001b[1m\u001b[34md\u001b[0mh\u001b[1m\u001b[31m \u001b[0m\n",
      " \n",
      "Ground Truth            karaṇakatvasaṃbaṃdhenapratyayārthopradhān\u001b[1m\u001b[33ma\u001b[0mnvayamapekṣyasvāśrayakaraṇakatvasaṃbaṃdhen\u001b[1m\u001b[31me\u001b[0m\u001b[1m\u001b[31mt\u001b[0m\u001b[1m\u001b[31my\u001b[0ma\u001b[1m\u001b[34m \u001b[0m\u001b[1m\u001b[34m \u001b[0m\u001b[1m\u001b[34m \u001b[0mbhyarhitenānvayaḥsiddh\u001b[1m\u001b[31me\u001b[0m\n",
      "Finetuned Model         karaṇakatvasaṃbaṃdhenapratyayārthopradhān\u001b[1m\u001b[33me\u001b[0mnvayamapekṣyasvāśrayakaraṇakatvasaṃbaṃdhen\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0ma\u001b[1m\u001b[34mi\u001b[0m\u001b[1m\u001b[34mv\u001b[0m\u001b[1m\u001b[34mā\u001b[0mbhyarhitenānvayaḥsiddh\u001b[1m\u001b[31m \u001b[0m\n",
      " \n",
      "Ground Truth            \u001b[1m\u001b[31mR\u001b[0m\u001b[1m\u001b[31m \u001b[0mkaraṇakatva\u001b[1m\u001b[31m_\u001b[0msa\u001b[1m\u001b[33mm\u001b[0mba\u001b[1m\u001b[33mn\u001b[0mdhena\u001b[1m\u001b[31m_\u001b[0mpratyay\u001b[1m\u001b[33ma\u001b[0m\u001b[1m\u001b[31m_\u001b[0m\u001b[1m\u001b[31ma\u001b[0mrth\u001b[1m\u001b[33ma\u001b[0m\u001b[1m\u001b[31mḥ\u001b[0m\u001b[1m\u001b[31m_\u001b[0mpradhān\u001b[1m\u001b[33ma\u001b[0m\u001b[1m\u001b[31m_\u001b[0mnvayam\u001b[1m\u001b[31m_\u001b[0mapekṣya\u001b[1m\u001b[31m_\u001b[0msvāśraya\u001b[1m\u001b[31mm\u001b[0m\u001b[1m\u001b[31m_\u001b[0mkaraṇakatva\u001b[1m\u001b[31m_\u001b[0msa\u001b[1m\u001b[33mm\u001b[0mba\u001b[1m\u001b[33mn\u001b[0mdhena\u001b[1m\u001b[34m \u001b[0m\u001b[1m\u001b[34m \u001b[0m\u001b[1m\u001b[33m_\u001b[0m\u001b[1m\u001b[31mī\u001b[0m\u001b[1m\u001b[31mś\u001b[0m\u001b[1m\u001b[31ma\u001b[0m\u001b[1m\u001b[31m_\u001b[0m\u001b[1m\u001b[31ma\u001b[0mbhyarhi\u001b[1m\u001b[31mḥ\u001b[0m\u001b[1m\u001b[31m_\u001b[0m\u001b[1m\u001b[31ma\u001b[0mten\u001b[1m\u001b[33ma\u001b[0m\u001b[1m\u001b[31m_\u001b[0m\u001b[1m\u001b[31ma\u001b[0mnvayaḥ\u001b[1m\u001b[31m_\u001b[0msid\u001b[1m\u001b[34m \u001b[0mh\u001b[1m\u001b[31me\u001b[0m\u001b[1m\u001b[31m_\u001b[0m\n",
      "Base Model              \u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0mkaraṇakatva\u001b[1m\u001b[31m \u001b[0msa\u001b[1m\u001b[33mṃ\u001b[0mba\u001b[1m\u001b[33mṃ\u001b[0mdhena\u001b[1m\u001b[31m \u001b[0mpratyay\u001b[1m\u001b[33mā\u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0mrth\u001b[1m\u001b[33mo\u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0mpradhān\u001b[1m\u001b[33me\u001b[0m\u001b[1m\u001b[31m \u001b[0mnvayam\u001b[1m\u001b[31m \u001b[0mapekṣya\u001b[1m\u001b[31m \u001b[0msvāśraya\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0mkaraṇakatva\u001b[1m\u001b[31m \u001b[0msa\u001b[1m\u001b[33mṃ\u001b[0mba\u001b[1m\u001b[33mṃ\u001b[0mdhena\u001b[1m\u001b[34mi\u001b[0m\u001b[1m\u001b[34mv\u001b[0m\u001b[1m\u001b[33mā\u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0mbhyarhi\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0mten\u001b[1m\u001b[33mā\u001b[0m\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0mnvayaḥ\u001b[1m\u001b[31m \u001b[0msid\u001b[1m\u001b[34md\u001b[0mh\u001b[1m\u001b[31m \u001b[0m\u001b[1m\u001b[31m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_in = 0\n",
    "_out = -1\n",
    "\n",
    "highlight_changes(tgt_text[_in:_out],tgt_text[_in:_out],\"Ground Truth\")\n",
    "print(' ')\n",
    "highlight_changes(tgt_text[_in:_out],ocr_text[_in:_out], \"OCR Output\")\n",
    "print(' ')\n",
    "highlight_changes(tgt_text[_in:_out],generate_text(pretrained_model,tokenizer,ocr_text[_in:_out]),\"Pretrained Model\")\n",
    "print(' ')\n",
    "highlight_changes(tgt_text[_in:_out],generate_text(finetuned_model,tokenizer,ocr_text[_in:_out]),\"Finetuned Model\")\n",
    "print(' ')\n",
    "highlight_changes(tgt_text[_in:_out],generate_text(base_model,tokenizer,ocr_text[_in:_out]),\"Base Model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ā</s>']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([tokenizer('ā').input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ocr_text \u001b[38;5;241m=\u001b[39m \u001b[43mocr_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mline_number\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m tgt_text \u001b[38;5;241m=\u001b[39m target_list[line_number]\n\u001b[1;32m      3\u001b[0m i \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "ocr_text = ocr_list[line_number]\n",
    "tgt_text = target_list[line_number]\n",
    "i =30\n",
    "j=40\n",
    "\n",
    "input_ids = tokenizer(ocr_text).input_ids\n",
    "pre = tokenizer.batch_decode([input_ids[:i]])\n",
    "mask = tokenizer.batch_decode([input_ids[i:j]])\n",
    "post = tokenizer.batch_decode([input_ids[j:]])\n",
    "print(pre)\n",
    "print(mask)\n",
    "#print(post)\n",
    "\n",
    "input_ids_tensor = torch.tensor([input_ids[:i] + [258] + input_ids[j:]])\n",
    "output_ids = base_model.generate(input_ids_tensor, max_length=10)[0].tolist()\n",
    "print(tokenizer.batch_decode([output_ids]))\n",
    "line_number+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>R virodha']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kāsenet']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     sentinel_token \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     10\u001b[0m output_ids_list\u001b[38;5;241m.\u001b[39mappend(output_ids[start_token:])\n\u001b[0;32m---> 11\u001b[0m output_string \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_ids_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ByT5/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3867\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3842\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3843\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3844\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3847\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3848\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3849\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3850\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3851\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3864\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3865\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3867\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3871\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3873\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3874\u001b[0m     ]\n",
      "File \u001b[0;32m~/.conda/envs/ByT5/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3906\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3903\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3904\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ByT5/lib/python3.12/site-packages/transformers/tokenization_utils.py:1072\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1064\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1069\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1072\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m   1074\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1075\u001b[0m     }\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ByT5/lib/python3.12/site-packages/transformers/tokenization_utils.py:1047\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m   1045\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m-> 1047\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "output_ids_list = []\n",
    "start_token = 0\n",
    "sentinel_token = 258\n",
    "while sentinel_token in output_ids:\n",
    "    split_idx = output_ids.index(sentinel_token)\n",
    "    output_ids_list.append(output_ids[start_token:split_idx])\n",
    "    start_token = split_idx\n",
    "    sentinel_token -= 1\n",
    "\n",
    "output_ids_list.append(output_ids[start_token:])\n",
    "output_string = tokenizer.batch_decode([output_ids_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>R saṃkhyāyāḥ_saṃkhyāyāḥ_saṃkhyāyāḥ_saṃkhyāyāḥ_</s>']\n"
     ]
    }
   ],
   "source": [
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'panvayenadhātvarathenanvayāt.saṃkhyāyāapyupadārthatavāviśeghāt.tenasākṣāhmaṃkhyānvayā..vanābheṃda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ByT5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
